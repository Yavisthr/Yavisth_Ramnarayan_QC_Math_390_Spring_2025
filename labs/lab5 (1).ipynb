{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b1538b0-f92b-48b9-be07-e9527576bc8b",
   "metadata": {},
   "source": [
    "## Course Assignment Instructions\n",
    "You should have Python (version 3.8 or later) and Jupyter Notebook installed to complete this assignment. You will write code in the empty cell/cells below the problem. While most of this will be a programming assignment, some questions will ask you to \"write a few sentences\" in markdown cells. \n",
    "\n",
    "Submission Instructions:\n",
    "\n",
    "Create a labs directory in your personal class repository (e.g., located in your home directory)\n",
    "Clone the class repository\n",
    "Copy this Jupyter notebook file (.ipynb) into your repo/labs directory\n",
    "Make your edits, commit changes, and push to your repository\n",
    "All submissions must be pushed before the due date to avoid late penalties. \n",
    "\n",
    "Labs are graded out of a 100 pts. Each day late is -10. For a max penalty of -50 after 5 days. From there you may submit the lab anytime before the semester ends for a max score of 50.  \n",
    "\n",
    "Lab 5 is due on 3/17/25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa0e89f-550b-4278-90c1-5b4d73c3d088",
   "metadata": {},
   "source": [
    "Write a function spec'd as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80200021-19e2-4bd1-bd14-705326161aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def norm_vec(x):\n",
    "    \"\"\"Compute the Euclidean norm of a vector.\"\"\"\n",
    "    return np.sqrt(np.sum(x**2))\n",
    "\n",
    "def orthogonal_projection(a, v):\n",
    "    \"\"\"\n",
    "    Projects vector a onto vector v.\n",
    "    \n",
    "    Parameters:\n",
    "        a (numpy.ndarray): The vector to project.\n",
    "        v (numpy.ndarray): The vector to project onto.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with keys:\n",
    "            'a_parallel': the projection of a onto v,\n",
    "            'a_perpendicular': the residual component of a orthogonal to v.\n",
    "    \"\"\"\n",
    "    # Ensure that a and v are numpy arrays\n",
    "    a = np.asarray(a)\n",
    "    v = np.asarray(v)\n",
    "    \n",
    "    # Compute the squared norm of v\n",
    "    v_norm_sq = norm_vec(v)**2\n",
    "    \n",
    "    # Create the projection matrix using the outer product of v\n",
    "    H = np.outer(v, v) / v_norm_sq\n",
    "    \n",
    "    # Compute the parallel component of a\n",
    "    a_parallel = H @ a\n",
    "    \n",
    "    # Compute the orthogonal (perpendicular) component\n",
    "    a_perpendicular = a - a_parallel\n",
    "    \n",
    "    return {'a_parallel': a_parallel, 'a_perpendicular': a_perpendicular}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923afa28-a1f6-46e8-bb9c-b51c427752b3",
   "metadata": {},
   "source": [
    "Provide predictions for each of these computations and then run them to make sure you're correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bf93752-a0a0-42e0-975a-43f01e967011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: orthogonal_projection([1,2,3,4], [1,2,3,4])\n",
      "a_parallel:\n",
      "[1. 2. 3. 4.]\n",
      "a_perpendicular:\n",
      "[0. 0. 0. 0.]\n",
      "\n",
      "Test 2: orthogonal_projection([1,2,3,4], [0,2,0,-1])\n",
      "a_parallel:\n",
      "[0. 0. 0. 0.]\n",
      "a_perpendicular:\n",
      "[1. 2. 3. 4.]\n",
      "\n",
      "Test 3: result = orthogonal_projection([2,6,7,3], [1,3,5,7]*37)\n",
      "Dot product (should be near 0): -5.329070518200751e-15\n",
      "Sum (should equal original vector [2,6,7,3]):\n",
      "[2. 5. 7. 3.]\n",
      "a_parallel divided by ([1,3,5,7]*37) (percentage of projection):\n",
      "[0.02348777 0.02348777 0.02348777 0.02348777]\n"
     ]
    }
   ],
   "source": [
    "# Test 1:\n",
    "print(\"Test 1: orthogonal_projection([1,2,3,4], [1,2,3,4])\")\n",
    "result1 = orthogonal_projection([1,2,3,4], [1,2,3,4])\n",
    "print(\"a_parallel:\")\n",
    "print(result1['a_parallel'])        # Expected: [1, 2, 3, 4]\n",
    "print(\"a_perpendicular:\")\n",
    "print(result1['a_perpendicular'])     # Expected: [0, 0, 0, 0]\n",
    "\n",
    "# Test 2:\n",
    "print(\"\\nTest 2: orthogonal_projection([1,2,3,4], [0,2,0,-1])\")\n",
    "result2 = orthogonal_projection([1,2,3,4], [0,2,0,-1])\n",
    "print(\"a_parallel:\")\n",
    "print(result2['a_parallel'])          # Expected: [0, 0, 0, 0]\n",
    "print(\"a_perpendicular:\")\n",
    "print(result2['a_perpendicular'])       # Expected: [1, 2, 3, 4]\n",
    "\n",
    "# Test 3:\n",
    "print(\"\\nTest 3: result = orthogonal_projection([2,6,7,3], [1,3,5,7]*37)\")\n",
    "# Multiply [1,3,5,7] by 37 and project\n",
    "v3 = np.array([1,3,5,7])*37 \n",
    "result3 = orthogonal_projection([2, 5, 7, 3], v3)\n",
    "\n",
    "# Compute dot product between a_parallel and a_perpendicular (should be near 0)\n",
    "dot_product = np.dot(result3['a_parallel'], result3['a_perpendicular'])\n",
    "print(\"Dot product (should be near 0):\", dot_product)\n",
    "\n",
    "# Sum of parallel and perpendicular components should return the original vector.\n",
    "sum_vector = result3['a_parallel'] + result3['a_perpendicular']\n",
    "print(\"Sum (should equal original vector [2,6,7,3]):\")\n",
    "print(sum_vector)\n",
    "\n",
    "# Compute the \"percentage\" of the projection relative to v3.\n",
    "percentage = result3['a_parallel'] / v3\n",
    "print(\"a_parallel divided by ([1,3,5,7]*37) (percentage of projection):\")\n",
    "print(percentage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998085c4-9b18-402e-a8b4-4a91cc882b46",
   "metadata": {},
   "source": [
    "Create a vector y by simulating n = 100 standard iid normals. Create a matrix of size 100 x 2 and populate the first column by all ones (for the intercept) and the second column by 100 standard iid normals. Find the R^2 of an OLS regression of `y ~ X`. Use matrix algebra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f1779d5-c6e5-4670-832a-a47634752b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSR: 127.24524905012579\n",
      "SST: 127.2926559224331\n",
      "R^2: 0.0003724242531022437\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(123)\n",
    "\n",
    "# Simulate data\n",
    "n = 100\n",
    "y = np.random.randn(n, 1)  # y as a column vector (100 x 1)\n",
    "X = np.hstack((np.ones((n, 1)), np.random.randn(n, 1)))  # X as 100 x 2 matrix\n",
    "\n",
    "# Compute OLS coefficients: beta_hat = (X'X)^{-1} X'y\n",
    "beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "# Compute the hat matrix H = X (X'X)^{-1} X'\n",
    "H = X @ np.linalg.inv(X.T @ X) @ X.T\n",
    "\n",
    "# Identity matrix of size n\n",
    "I = np.eye(n)\n",
    "\n",
    "# Compute SSR (Residual Sum of Squares) using matrix algebra: SSR = y' (I - H) y\n",
    "SSR = (y.T @ (I - H) @ y).item() #.item() will grab the value out of the array \n",
    "\n",
    "# Define the centering matrix: M = I - (1/n) * 11'\n",
    "M = I - (1/n) * np.ones((n, n))\n",
    "\n",
    "# Compute SST (Total Sum of Squares) using matrix algebra: SST = y' M y\n",
    "SST = (y.T @ M @ y).item() \n",
    "\n",
    "# Calculate R-squared\n",
    "R2 = 1 - SSR / SST\n",
    "\n",
    "# Print the results\n",
    "print(\"SSR:\", SSR)\n",
    "print(\"SST:\", SST)\n",
    "print(\"R^2:\", R2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b19394-0558-4670-86e0-2d9295ae1243",
   "metadata": {},
   "source": [
    "Write a for loop to each time bind a new column of 100 standard iid normals to the matrix X and find the R^2 each time until the number of columns is 100. Create a vector to save all R^2's. What happened??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "736b27a0-df6f-447c-8e78-f3c443e2e3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 values for models with 1 to 100 columns:\n",
      "[0.0, 0.00037242425310191063, 0.0039534712234703395, 0.015515917195883744, 0.0160129569554901, 0.060483757420479844, 0.0604856345047039, 0.06791713923101828, 0.11552054867820061, 0.11598543147994111, 0.16253199145134212, 0.16324852246361643, 0.1653058735950539, 0.16726405487373375, 0.16959752126125305, 0.1867916546549011, 0.18680390934298896, 0.19280704730131826, 0.19656454956494707, 0.20676806135980885, 0.2067805910051016, 0.21297298468610126, 0.21413227261249568, 0.22344231007274185, 0.2644021621922207, 0.2688518991099367, 0.2904508089901391, 0.29132140616382174, 0.2940661523691508, 0.29505341769528637, 0.3224612676383528, 0.34069205476632747, 0.34966372664455914, 0.36519299490256085, 0.36609673260436104, 0.38741665786176294, 0.4030608157085439, 0.40637996512050323, 0.4084185281712275, 0.4086532629450572, 0.4087302243058516, 0.4122560226143148, 0.4199723297044975, 0.4529627223389995, 0.45435748259751674, 0.4583895135974029, 0.4584411626637773, 0.4587105682059598, 0.45982371981347814, 0.4600827673875302, 0.464215577201027, 0.46568539273667275, 0.4740253125991649, 0.4759792162552864, 0.4767914615445271, 0.478609966753774, 0.48384856587282143, 0.4841312500428516, 0.4861020907809164, 0.48632682326221577, 0.48719767430164596, 0.4872726576033761, 0.48810957144301814, 0.4914336154309201, 0.5026734179392027, 0.5218393708002693, 0.5226854273997588, 0.5306612598397726, 0.5400117729727334, 0.5758618965548252, 0.5794672894601183, 0.5825674203353445, 0.5833897760854554, 0.597785586429479, 0.6020719460622732, 0.6329237023727555, 0.6428928919050505, 0.6792167766016535, 0.6804377097045042, 0.6836940953170598, 0.687726217031866, 0.6958153024269472, 0.6959141060332985, 0.8269638581828747, 0.8269642644890395, 0.8421062351150496, 0.8511300271839719, 0.8526653897465583, 0.8912768916579414, 0.8935047892458806, 0.9170216179578101, 0.9194927487245198, 0.919861978582901, 0.9290091629043679, 0.9358374484878511, 0.9528344080343918, 0.969436419641471, 0.9746421719572972, 0.998360377510893, 1.0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(123)\n",
    "\n",
    "# Simulate data\n",
    "n = 100\n",
    "y = np.random.randn(n)  # response vector (100,)\n",
    "\n",
    "# Start with an intercept-only matrix X (100 x 1)\n",
    "X = np.ones((n, 1))\n",
    "\n",
    "# Pre-allocate a list to save R^2 values (one for each model size)\n",
    "R2_list = []\n",
    "\n",
    "# Loop until the number of columns in X is 100\n",
    "# First iteration: intercept only; then add one new column per iteration.\n",
    "for i in range(100):\n",
    "    # For iterations after the first, bind a new column of iid normals.\n",
    "    if i > 0:\n",
    "        new_column = np.random.randn(n, 1)\n",
    "        X = np.hstack((X, new_column))\n",
    "    \n",
    "    # Compute OLS coefficients using the normal equation: beta_hat = (X'X)^{-1} X'y\n",
    "    beta_hat = np.linalg.inv(X.T @ X) @ (X.T @ y)\n",
    "    \n",
    "    # Compute fitted values\n",
    "    y_hat = X @ beta_hat\n",
    "    \n",
    "    # Compute Total Sum of Squares (SST) and Residual Sum of Squares (SSR)\n",
    "    SST = np.sum((y - np.mean(y))**2)\n",
    "    SSR = np.sum((y - y_hat)**2)\n",
    "    \n",
    "    # Calculate R-squared and store it\n",
    "    R2 = 1 - SSR / SST\n",
    "    R2_list.append(R2)\n",
    "\n",
    "# Print the vector of R^2 values\n",
    "print(\"R^2 values for models with 1 to 100 columns:\")\n",
    "print(R2_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81287175-3680-42bb-9125-67b175c53424",
   "metadata": {},
   "source": [
    "What happened? As more predictors are added, even if they are just noise, the R2 generally increases, relfecting that the model is overfitting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e99824f-7049-4057-bfa8-5659a13e3aa4",
   "metadata": {},
   "source": [
    "Test that the projection matrix onto this X is the same as I_n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50f24d3f-c93d-4104-937d-e4ec12a072e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projection matrix P equals the identity matrix I_n.\n"
     ]
    }
   ],
   "source": [
    "# Compute the projection matrix: P = X (X'X)^{-1} X'\n",
    "P = X @ np.linalg.inv(X.T @ X) @ X.T\n",
    "\n",
    "# Define the 100 x 100 identity matrix\n",
    "I_n = np.eye(n)\n",
    "\n",
    "# Test that the projection matrix P equals the identity matrix.\n",
    "if np.allclose(P, I_n):\n",
    "    print(\"Projection matrix P equals the identity matrix I_n.\")\n",
    "else:\n",
    "    print(\"Projection matrix P does NOT equal the identity matrix I_n.\")\n",
    "\n",
    "# Alternatively, use an assertion to ensure they are equal\n",
    "assert np.allclose(P, I_n), \"Projection matrix does not equal identity matrix!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0877001c-87e9-46f6-ae2f-29a265b9ba43",
   "metadata": {},
   "source": [
    "Add one final column to X to bring the number of columns to 101. Then try to compute R^2. What happens? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb942568-2bf2-4960-99dc-37a2c39d5a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.8115558321599017\n"
     ]
    }
   ],
   "source": [
    "# Now add one final column to bring the number of columns to 101.\n",
    "extra_col = np.random.randn(n, 1)\n",
    "X_new = np.hstack((X, extra_col))  # X_new is 100 x 101\n",
    "\n",
    "# Try to compute OLS coefficients; catch the error if X.T @ X is singular.\n",
    "try:\n",
    "    beta_hat = np.linalg.inv(X_new.T @ X_new) @ (X_new.T @ y)\n",
    "    y_hat = X_new @ beta_hat\n",
    "    SST = np.sum((y - np.mean(y))**2)\n",
    "    SSR = np.sum((y - y_hat)**2)\n",
    "    R2 = 1 - SSR/SST\n",
    "    print(\"R^2:\", R2)\n",
    "except np.linalg.LinAlgError as e:\n",
    "    print(\"Error in computing beta_hat:\",)\n",
    "    print(\"Could not compute R^2 because X'X is singular.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2564094-7ac8-4061-adea-fbdaa1a79a82",
   "metadata": {},
   "source": [
    "Why does this make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6b14cc-d0b1-4d19-9633-1cdff50bf579",
   "metadata": {},
   "source": [
    "X has 101 columns but only 100 rows making the XTX matrix singular because of this linearly dependent column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73d693a-5e8d-4efd-8dd7-ee5b840584b9",
   "metadata": {},
   "source": [
    "Let's use the Boston Housing Data for the following exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f9e3875-f26e-4e36-addb-dc79442a6fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictors (including intercept): 14\n",
      "Number of observations: 506\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load the Boston housing data from the provided URL\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=r\"\\s+\", skiprows=22, header=None)\n",
    "\n",
    "# The dataset from this URL is arranged so that every two rows correspond to one observation:\n",
    "# - The first row of each pair contains the 13 predictors.\n",
    "# - The second row contains (among other entries) the response (medv) in the third column.\n",
    "# Also, the first two columns of the second row are duplicates of the last two predictors.\n",
    "#\n",
    "# To mimic R’s model.matrix(medv ~ ., MASS::Boston), we construct the predictors by:\n",
    "# 1. Taking all 13 predictors from the first row of each pair.\n",
    "# 2. Appending the first two entries from the second row (which complete the predictors).\n",
    "#\n",
    "# This creates a full set of predictors for each observation.\n",
    "X_data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "\n",
    "# The response y is stored in the third column of the second row of each pair.\n",
    "y = raw_df.values[1::2, 2]\n",
    "\n",
    "# Add an intercept column (like R's model.matrix does automatically)\n",
    "X = sm.add_constant(X_data)\n",
    "\n",
    "# p_plus_one is the number of columns in X (predictors plus intercept)\n",
    "p_plus_one = X.shape[1]\n",
    "# n is the number of observations (rows in X)\n",
    "n = X.shape[0]\n",
    "\n",
    "print(\"Number of predictors (including intercept):\", p_plus_one)\n",
    "print(\"Number of observations:\", n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7c1a4b-da46-4b6d-a118-6d555303ef15",
   "metadata": {},
   "source": [
    "Using your function `orthogonal_projection` orthogonally project onto the column space of X by projecting y on each vector of X individually and adding up the projections and call the sum `yhat_naive`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6877ea9-b7b7-45a0-a7aa-e3eea9911088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize yhat_naive as a zero vector (same shape as y)\n",
    "yhat_naive = np.zeros_like(y, dtype = float)\n",
    "\n",
    "# For each column in X, project y onto that column and add the projection.\n",
    "for j in range(p_plus_one):\n",
    "    # Note: X[:, j] is a 1D array of length n.\n",
    "    proj = orthogonal_projection(y, X[:, j])\n",
    "    yhat_naive += proj['a_parallel']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f46e13e-3846-4cae-904e-05db448815fb",
   "metadata": {},
   "source": [
    "How much double counting occurred? Measure the magnitude relative to the true LS orthogonal projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b81a208-d76e-4b52-9df6-d828148b6f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magnitude ratio (sqrt(sum(yhat_naive^2))/sqrt(sum(yhat^2))): 8.997117717661743\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Fit the OLS model using statsmodels\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Extract the fitted values (equivalent to lm(medv ~ ., MASS::Boston)$fitted.values in R)\n",
    "yhat = model.fittedvalues\n",
    "mag_ratio = np.sqrt(np.sum(yhat_naive**2))/np.sqrt(np.sum(yhat**2))\n",
    "print(\"Magnitude ratio (sqrt(sum(yhat_naive^2))/sqrt(sum(yhat^2))):\", mag_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db340f11-194f-4236-9eb4-b02385a62027",
   "metadata": {},
   "source": [
    "Is this ratio expected? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf111525-6ff3-4f23-a2b0-ee48e8fdba59",
   "metadata": {},
   "source": [
    "In summary, the ratio is expected to deviate from 1 because the sum of individual (non-orthogonal) projections overestimates the overall projection onto the column space, leading to a larger magnitude for the naive projection compared to the true LS projection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c61bf97-44ee-41ac-a3bb-ccbebb3c3dd5",
   "metadata": {},
   "source": [
    "Convert X into V where V has the same column space as X but has orthogonal columns. You can use the function `orthogonal_projection`. This is the Gram-Schmidt orthogonalization algorithm (part A)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7c953a6-710b-4565-83b2-08c651f92490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize V to hold the orthogonalized columns; same shape as X.\n",
    "V = np.empty_like(X)\n",
    "\n",
    "# The first column of V is just the first column of X.\n",
    "V[:, 0] = X[:, 0]\n",
    "\n",
    "# For each subsequent column of X, subtract the projections onto all previous V columns.\n",
    "for j in range(1, X.shape[1]):\n",
    "    # Start with the original column from X.\n",
    "    V[:, j] = X[:, j].copy()\n",
    "    for k in range(j):\n",
    "        proj = orthogonal_projection(X[:, j], V[:, k])['a_parallel']\n",
    "        V[:, j] = V[:, j] - proj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26ba41e-ec46-4966-9ae2-30b95dcd322b",
   "metadata": {},
   "source": [
    "Convert V into Q whose columns are the same except normalized. This is the Gram-Schmidt orthogonalization algorithm (part B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f9bbc2e-bacb-4106-856a-74013615c745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume V is already defined with shape (n, p_plus_one)\n",
    "Q = np.empty_like(V)\n",
    "for j in range(V.shape[1]):\n",
    "    Q[:, j] = V[:, j] / np.sqrt(np.sum(V[:, j]**2))\n",
    "# Optionally, remove V if you no longer need it.\n",
    "del V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c587a626-aae5-4295-81a4-c014d55a0441",
   "metadata": {},
   "source": [
    "Verify Q^T Q is I_{p+1} i.e. Q is an orthonormal matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c70bb05-8afa-4f6d-9e6e-6c365a113ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q^T Q equals the identity matrix, Q is orthonormal.\n"
     ]
    }
   ],
   "source": [
    "# Verify that Q^T Q is the identity matrix of size p_plus_one\n",
    "p_plus_one = Q.shape[1]\n",
    "\n",
    "assert np.allclose(Q.T @ Q, np.eye(p_plus_one))\n",
    "\n",
    "print(\"Q^T Q equals the identity matrix, Q is orthonormal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad012fab-f233-4a7d-b8b4-c7d5030b1f97",
   "metadata": {},
   "source": [
    "Is your Q the same as what results from R's built-in QR-decomposition function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26df14a5-db7d-4baf-bf3f-ac11a70521fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q matches the built-in QR decomposition Q (up to sign differences).\n"
     ]
    }
   ],
   "source": [
    "# Compute the Q matrix using NumPy's QR decomposition.\n",
    "# np.linalg.qr returns (Q, R)\n",
    "Q_from_builtin, X = np.linalg.qr(X)\n",
    "# Because QR decomposition can produce Q whose columns differ by a sign,\n",
    "# we compare the absolute values of Q and Q_from_builtin.\n",
    "if np.allclose(np.abs(Q), np.abs(Q_from_builtin)):\n",
    "    print(\"Q matches the built-in QR decomposition Q (up to sign differences).\")\n",
    "else:\n",
    "    print(\"Q does not match the built-in QR decomposition Q.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2b6ee10-d06e-4f31-9f16-af11a9775dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q does not match the built-in QR decomposition Q.\n"
     ]
    }
   ],
   "source": [
    "if np.allclose(Q, Q_from_builtin): \n",
    "    print(\"Q matches the built-in QR decomposition Q (up to sign differences).\")\n",
    "else:\n",
    "    print(\"Q does not match the built-in QR decomposition Q.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190ac792-5397-437d-9282-6516ae05e7e2",
   "metadata": {},
   "source": [
    "Is this expected? Why did this happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ee2c88-f5dc-4c7d-8241-eb9f2295c407",
   "metadata": {},
   "source": [
    "Yes this is expected because there are an infinite number of orthonormal basis of any column space and the likelihood of them being equal is highly unlikely.  \n",
    "There are many different orthonormal basis of any column space. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91635448-669c-4f1a-ab75-4fe76c5f339a",
   "metadata": {},
   "source": [
    "Project y onto colsp[Q] and verify it is the same as the OLS fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a6ddf2c-c1bb-4c6a-8023-e2e7666ad9cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "endog and exog matrices are different sizes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Assume y, X, and Q are already defined.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Compute the OLS fitted values using statsmodels (similar to lm(y ~ X) in R)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39mOLS(y, X)\u001b[38;5;241m.\u001b[39mfit()\n\u001b[0;32m      4\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m modle\u001b[38;5;241m.\u001b[39mfittedvalues\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Project y onto the column space of Q: compute Q @ Q.T @ y\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Downloads\\ansa\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:924\u001b[0m, in \u001b[0;36mOLS.__init__\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    921\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights are not supported in OLS and will be ignored\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    922\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn exception will be raised in the next version.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    923\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, ValueWarning)\n\u001b[1;32m--> 924\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, missing\u001b[38;5;241m=\u001b[39mmissing,\n\u001b[0;32m    925\u001b[0m                           hasconst\u001b[38;5;241m=\u001b[39mhasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_keys:\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_keys\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Downloads\\ansa\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:749\u001b[0m, in \u001b[0;36mWLS.__init__\u001b[1;34m(self, endog, exog, weights, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     weights \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m--> 749\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, missing\u001b[38;5;241m=\u001b[39mmissing,\n\u001b[0;32m    750\u001b[0m                           weights\u001b[38;5;241m=\u001b[39mweights, hasconst\u001b[38;5;241m=\u001b[39mhasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    751\u001b[0m nobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    752\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\n",
      "File \u001b[1;32mD:\\Downloads\\ansa\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:203\u001b[0m, in \u001b[0;36mRegressionModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpinv_wexog: Float64Array \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_attr\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpinv_wexog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwendog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwexog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mD:\\Downloads\\ansa\\Lib\\site-packages\\statsmodels\\base\\model.py:270\u001b[0m, in \u001b[0;36mLikelihoodModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize()\n",
      "File \u001b[1;32mD:\\Downloads\\ansa\\Lib\\site-packages\\statsmodels\\base\\model.py:95\u001b[0m, in \u001b[0;36mModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m missing \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     94\u001b[0m hasconst \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhasconst\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_data(endog, exog, missing, hasconst,\n\u001b[0;32m     96\u001b[0m                               \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mk_constant\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexog\n",
      "File \u001b[1;32mD:\\Downloads\\ansa\\Lib\\site-packages\\statsmodels\\base\\model.py:135\u001b[0m, in \u001b[0;36mModel._handle_data\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_handle_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, missing, hasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 135\u001b[0m     data \u001b[38;5;241m=\u001b[39m handle_data(endog, exog, missing, hasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# kwargs arrays could have changed, easier to just attach here\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[1;32mD:\\Downloads\\ansa\\Lib\\site-packages\\statsmodels\\base\\data.py:675\u001b[0m, in \u001b[0;36mhandle_data\u001b[1;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    672\u001b[0m     exog \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(exog)\n\u001b[0;32m    674\u001b[0m klass \u001b[38;5;241m=\u001b[39m handle_data_class_factory(endog, exog)\n\u001b[1;32m--> 675\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m klass(endog, exog\u001b[38;5;241m=\u001b[39mexog, missing\u001b[38;5;241m=\u001b[39mmissing, hasconst\u001b[38;5;241m=\u001b[39mhasconst,\n\u001b[0;32m    676\u001b[0m              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Downloads\\ansa\\Lib\\site-packages\\statsmodels\\base\\data.py:89\u001b[0m, in \u001b[0;36mModelData.__init__\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_constant(hasconst)\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity()\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mD:\\Downloads\\ansa\\Lib\\site-packages\\statsmodels\\base\\data.py:436\u001b[0m, in \u001b[0;36mModelData._check_integrity\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendog):\n\u001b[1;32m--> 436\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mendog and exog matrices are different sizes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: endog and exog matrices are different sizes"
     ]
    }
   ],
   "source": [
    "# Assume y, X, and Q are already defined.\n",
    "# Compute the OLS fitted values using statsmodels (similar to lm(y ~ X) in R)\n",
    "model = sm.OLS(y, X).fit()\n",
    "y_hat = modle.fittedvalues\n",
    "# Project y onto the column space of Q: compute Q @ Q.T @ y\n",
    "y_hat_from_Q = Q @ Q.T @ y\n",
    "\n",
    "# Compare the two. Since the values may have slight numerical differences, use np.allclose.\n",
    "if  np.allclose(y_hat_from_Q, y_hat):\n",
    "    print(\"The projection of y onto colsp[Q] equals the OLS fit.\")\n",
    "else:\n",
    "    print(\"There is a discrepancy between the projection and the OLS fit.\")\n",
    "\n",
    "# Optionally, you can print the difference:\n",
    "print(\"Difference:\", y_hat_from_Q - y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33be031-6ec3-4d45-b74c-baf3269d0997",
   "metadata": {},
   "source": [
    "Project y onto colsp[Q] one by one and verify it sums to be the projection onto the whole space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17ec740d-6bd8-4155-ad3d-203a277984ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The naive projection equals the full projection.\n",
      "Difference: [-1.06581410e-14 -1.42108547e-14 -2.13162821e-14 -1.42108547e-14\n",
      " -1.42108547e-14 -1.06581410e-14 -2.48689958e-14 -2.13162821e-14\n",
      " -1.59872116e-14 -1.77635684e-14 -1.42108547e-14 -1.06581410e-14\n",
      " -2.48689958e-14 -1.77635684e-14 -1.77635684e-14 -2.13162821e-14\n",
      " -1.77635684e-14 -1.77635684e-14 -1.42108547e-14 -1.77635684e-14\n",
      " -2.30926389e-14 -1.06581410e-14 -1.06581410e-14 -1.06581410e-14\n",
      " -8.88178420e-15 -1.42108547e-14 -7.10542736e-15 -1.95399252e-14\n",
      " -1.77635684e-14  0.00000000e+00 -1.42108547e-14 -1.77635684e-14\n",
      " -1.59872116e-14 -1.59872116e-14 -1.95399252e-14 -1.06581410e-14\n",
      " -1.06581410e-14 -1.42108547e-14 -1.42108547e-14 -1.06581410e-14\n",
      " -2.84217094e-14 -1.42108547e-14 -2.13162821e-14 -7.10542736e-15\n",
      " -1.77635684e-14 -3.55271368e-15 -1.06581410e-14 -1.42108547e-14\n",
      " -5.32907052e-15 -2.48689958e-14 -1.06581410e-14 -1.42108547e-14\n",
      " -1.77635684e-14 -1.42108547e-14 -1.77635684e-14 -1.77635684e-14\n",
      " -1.77635684e-14 -1.42108547e-14 -2.48689958e-14 -1.77635684e-14\n",
      " -2.13162821e-14 -1.77635684e-14 -1.42108547e-14 -1.42108547e-14\n",
      " -1.42108547e-14 -7.10542736e-15 -2.48689958e-14 -1.06581410e-14\n",
      " -1.06581410e-14 -1.06581410e-14 -3.55271368e-15 -1.42108547e-14\n",
      " -1.06581410e-14 -1.42108547e-14 -1.77635684e-14 -1.77635684e-14\n",
      " -1.42108547e-14 -1.77635684e-14 -1.42108547e-14 -1.77635684e-14\n",
      " -1.77635684e-14 -2.13162821e-14 -1.42108547e-14 -1.06581410e-14\n",
      " -2.13162821e-14 -2.13162821e-14 -1.42108547e-14 -2.48689958e-14\n",
      " -7.10542736e-15 -1.77635684e-14 -3.55271368e-14 -2.48689958e-14\n",
      " -7.10542736e-15 -2.13162821e-14 -2.13162821e-14 -1.42108547e-14\n",
      " -2.48689958e-14 -7.10542736e-15 -2.84217094e-14 -2.13162821e-14\n",
      " -1.77635684e-14 -1.42108547e-14 -1.42108547e-14 -1.42108547e-14\n",
      " -2.13162821e-14 -2.13162821e-14 -2.48689958e-14 -2.48689958e-14\n",
      " -1.77635684e-14 -1.77635684e-14 -1.77635684e-14 -1.77635684e-14\n",
      " -1.42108547e-14 -1.77635684e-14 -2.13162821e-14 -1.77635684e-14\n",
      " -2.13162821e-14 -2.13162821e-14 -1.42108547e-14 -7.10542736e-15\n",
      " -2.48689958e-14 -1.42108547e-14 -1.77635684e-14 -2.13162821e-14\n",
      " -2.13162821e-14 -2.48689958e-14 -2.30926389e-14 -2.30926389e-14\n",
      " -1.06581410e-14 -1.59872116e-14 -1.77635684e-14 -1.06581410e-14\n",
      " -2.13162821e-14 -1.24344979e-14 -1.95399252e-14 -1.77635684e-14\n",
      " -1.77635684e-14 -1.77635684e-14 -1.77635684e-14 -1.06581410e-14\n",
      " -1.77635684e-14 -1.24344979e-14 -1.24344979e-14 -1.42108547e-14\n",
      " -1.77635684e-14 -2.13162821e-14 -1.59872116e-14 -1.06581410e-14\n",
      " -1.59872116e-14 -1.77635684e-14 -1.42108547e-14 -1.77635684e-14\n",
      " -2.48689958e-14 -1.77635684e-14 -1.77635684e-14 -2.13162821e-14\n",
      " -1.42108547e-14 -1.42108547e-14 -2.13162821e-14 -1.77635684e-14\n",
      " -2.13162821e-14 -1.42108547e-14 -1.42108547e-14 -2.84217094e-14\n",
      " -2.13162821e-14 -1.77635684e-14  0.00000000e+00 -2.48689958e-14\n",
      " -1.77635684e-14 -1.77635684e-14 -1.77635684e-14 -2.13162821e-14\n",
      " -1.42108547e-14 -1.77635684e-14 -2.48689958e-14 -2.13162821e-14\n",
      " -1.77635684e-14 -2.48689958e-14 -2.48689958e-14 -2.13162821e-14\n",
      " -2.13162821e-14 -7.10542736e-15 -1.42108547e-14 -1.77635684e-14\n",
      " -1.77635684e-14 -1.77635684e-14 -1.42108547e-14 -2.13162821e-14\n",
      " -2.84217094e-14 -3.55271368e-14 -1.77635684e-14 -1.77635684e-14\n",
      " -2.13162821e-14 -1.42108547e-14 -2.48689958e-14  0.00000000e+00\n",
      " -1.42108547e-14 -7.10542736e-15 -1.42108547e-14 -2.13162821e-14\n",
      " -1.06581410e-14 -2.13162821e-14 -7.10542736e-15 -1.42108547e-14\n",
      " -7.10542736e-15 -2.13162821e-14 -1.77635684e-14 -1.42108547e-14\n",
      " -2.13162821e-14 -2.13162821e-14 -2.84217094e-14 -1.77635684e-14\n",
      " -1.06581410e-14 -1.77635684e-14 -2.13162821e-14 -2.48689958e-14\n",
      " -1.42108547e-14 -1.42108547e-14 -1.77635684e-14 -1.77635684e-14\n",
      " -2.84217094e-14 -1.77635684e-14 -7.10542736e-15 -7.10542736e-15\n",
      " -1.42108547e-14 -2.84217094e-14 -2.84217094e-14 -1.42108547e-14\n",
      " -7.10542736e-15 -1.77635684e-14 -1.77635684e-14 -7.10542736e-15\n",
      " -1.42108547e-14 -7.10542736e-15 -1.06581410e-14 -1.77635684e-14\n",
      " -1.77635684e-14  0.00000000e+00 -1.77635684e-14 -1.42108547e-14\n",
      " -1.77635684e-14 -2.13162821e-14 -1.42108547e-14 -2.48689958e-14\n",
      " -1.77635684e-14 -1.24344979e-14 -1.77635684e-14 -1.42108547e-14\n",
      " -1.77635684e-14 -2.13162821e-14 -2.48689958e-14 -2.13162821e-14\n",
      " -1.77635684e-14 -1.42108547e-14 -2.13162821e-14 -2.13162821e-14\n",
      " -2.13162821e-14 -4.26325641e-14 -1.42108547e-14 -1.42108547e-14\n",
      " -1.42108547e-14 -2.13162821e-14 -7.10542736e-15 -2.13162821e-14\n",
      " -2.84217094e-14 -1.42108547e-14 -7.10542736e-15 -2.13162821e-14\n",
      " -1.42108547e-14 -2.13162821e-14 -2.13162821e-14 -1.77635684e-14\n",
      " -1.77635684e-14 -2.13162821e-14 -2.84217094e-14 -7.10542736e-15\n",
      " -1.42108547e-14 -2.13162821e-14 -2.13162821e-14 -1.42108547e-14\n",
      " -2.84217094e-14 -2.84217094e-14 -2.84217094e-14 -7.10542736e-15\n",
      " -7.10542736e-15 -2.13162821e-14 -1.42108547e-14 -2.48689958e-14\n",
      " -2.48689958e-14 -1.06581410e-14 -1.42108547e-14 -2.84217094e-14\n",
      " -3.19744231e-14 -1.42108547e-14 -1.42108547e-14 -1.06581410e-14\n",
      " -2.13162821e-14 -7.10542736e-15 -3.55271368e-15 -3.55271368e-15\n",
      " -1.42108547e-14 -2.48689958e-14 -1.77635684e-14 -1.42108547e-14\n",
      " -7.10542736e-15 -1.77635684e-14 -2.84217094e-14 -1.42108547e-14\n",
      " -1.77635684e-14 -1.42108547e-14 -2.13162821e-14 -1.42108547e-14\n",
      " -2.13162821e-14 -1.06581410e-14 -2.13162821e-14 -1.77635684e-14\n",
      " -2.13162821e-14 -1.06581410e-14 -1.77635684e-14 -1.77635684e-14\n",
      " -2.48689958e-14 -1.06581410e-14 -2.48689958e-14 -2.48689958e-14\n",
      " -1.77635684e-14 -1.42108547e-14 -1.06581410e-14 -2.13162821e-14\n",
      " -1.77635684e-14 -2.13162821e-14 -2.13162821e-14 -7.10542736e-15\n",
      " -1.77635684e-14 -1.77635684e-14 -1.06581410e-14 -7.10542736e-15\n",
      " -1.77635684e-14 -2.48689958e-14 -1.77635684e-14 -1.42108547e-14\n",
      " -7.10542736e-15 -2.13162821e-14 -1.42108547e-14 -3.55271368e-15\n",
      " -2.48689958e-14 -1.77635684e-14 -1.06581410e-14 -1.42108547e-14\n",
      " -1.42108547e-14 -1.42108547e-14 -1.42108547e-14 -2.13162821e-14\n",
      " -1.06581410e-14 -1.77635684e-14 -2.13162821e-14 -1.77635684e-14\n",
      " -1.42108547e-14 -1.77635684e-14 -7.10542736e-15 -1.42108547e-14\n",
      " -2.13162821e-14 -2.13162821e-14 -2.13162821e-14 -1.42108547e-14\n",
      " -3.55271368e-14 -1.77635684e-14 -1.24344979e-14 -1.24344979e-14\n",
      " -1.42108547e-14 -1.42108547e-14 -7.10542736e-15 -1.77635684e-14\n",
      " -2.13162821e-14 -8.88178420e-15 -1.42108547e-14 -1.42108547e-14\n",
      " -1.06581410e-14 -2.13162821e-14 -1.42108547e-14 -1.77635684e-14\n",
      " -2.13162821e-14 -1.42108547e-14 -1.42108547e-14 -2.13162821e-14\n",
      " -1.33226763e-14 -1.42108547e-14 -1.59872116e-14 -1.33226763e-14\n",
      " -1.59872116e-14 -1.59872116e-14 -2.48689958e-14 -1.77635684e-14\n",
      " -1.59872116e-14 -2.13162821e-14 -2.13162821e-14 -2.13162821e-14\n",
      " -2.13162821e-14 -1.77635684e-14 -1.33226763e-14 -1.95399252e-14\n",
      " -1.59872116e-14 -2.48689958e-14 -1.06581410e-14 -1.77635684e-14\n",
      " -7.10542736e-15 -1.06581410e-14 -1.77635684e-14 -1.77635684e-14\n",
      " -1.77635684e-14 -3.55271368e-15 -8.88178420e-15 -1.42108547e-14\n",
      " -1.59872116e-14 -1.77635684e-14 -1.06581410e-14 -1.42108547e-14\n",
      " -1.59872116e-14 -1.68753900e-14 -1.95399252e-14 -8.88178420e-15\n",
      " -2.13162821e-14 -1.42108547e-14 -1.77635684e-14 -1.95399252e-14\n",
      " -1.59872116e-14 -1.77635684e-14 -7.10542736e-15 -7.10542736e-15\n",
      " -1.77635684e-14 -8.88178420e-15 -1.06581410e-14 -1.77635684e-14\n",
      " -2.13162821e-14 -2.13162821e-14 -8.88178420e-15 -1.59872116e-14\n",
      " -2.13162821e-14 -2.13162821e-14 -1.33226763e-14 -1.77635684e-14\n",
      " -1.77635684e-14 -1.77635684e-14 -2.13162821e-14 -1.77635684e-14\n",
      " -1.42108547e-14 -1.24344979e-14 -1.42108547e-14 -1.77635684e-14\n",
      " -1.42108547e-14 -1.77635684e-14 -2.13162821e-14 -1.77635684e-14\n",
      " -1.42108547e-14 -1.42108547e-14 -1.77635684e-14 -1.59872116e-14\n",
      " -1.24344979e-14 -1.95399252e-14 -1.06581410e-14 -1.42108547e-14\n",
      " -1.77635684e-14 -1.77635684e-14 -1.42108547e-14 -1.42108547e-14\n",
      " -2.48689958e-14 -1.77635684e-14 -1.77635684e-14 -1.06581410e-14\n",
      " -1.77635684e-14 -1.77635684e-14 -1.42108547e-14 -2.48689958e-14\n",
      " -1.42108547e-14 -1.42108547e-14 -1.77635684e-14 -1.77635684e-14\n",
      " -1.77635684e-14 -2.30926389e-14 -1.77635684e-14 -1.42108547e-14\n",
      " -1.06581410e-14 -2.13162821e-14 -1.06581410e-14 -2.13162821e-14\n",
      " -2.48689958e-14 -2.13162821e-14 -1.77635684e-14 -7.10542736e-15\n",
      " -2.48689958e-14 -1.95399252e-14 -1.42108547e-14 -8.88178420e-15\n",
      " -1.24344979e-14 -2.48689958e-14 -1.77635684e-14 -2.13162821e-14\n",
      " -1.77635684e-14 -1.06581410e-14 -1.42108547e-14 -1.42108547e-14\n",
      " -1.77635684e-14 -1.06581410e-14 -1.42108547e-14 -2.13162821e-14\n",
      " -1.77635684e-14 -1.77635684e-14]\n"
     ]
    }
   ],
   "source": [
    "# Compute the naive projection: project y onto each column of Q and sum.\n",
    "yhat_naive = np.zeros_like(y)\n",
    "\n",
    "for j in range(p_plus_one):\n",
    "    proj = orthogonal_projection(y, Q[:, j])['a_parallel']\n",
    "    yhat_naive = yhat_naive + proj\n",
    "              \n",
    "# Compute the full projection using the projection matrix H.\n",
    "# Since Q is orthonormal, Q.T @ Q equals the identity matrix.\n",
    "H = Q @ Q.T  # Equivalent to Q @ np.linalg.inv(Q.T @ Q) @ Q.T\n",
    "\n",
    "# Verify that H @ y is the same as yhat_naive.\n",
    "if np.allclose(yhat_naive, H @ y):\n",
    "    print(\"The naive projection equals the full projection.\")\n",
    "else:\n",
    "    print(\"There is a discrepancy between the projections.\")\n",
    "\n",
    "# Optionally, to mimic R's expect_equal with unname:\n",
    "print(\"Difference:\", H @ y - yhat_naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bbb76d-1108-40bf-99c1-8eaf8ab3ba38",
   "metadata": {},
   "source": [
    "Split the Boston Housing Data into a training set and a test set where the training set is 80% of the observations. Do so at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59951734-a620-484b-9902-8bdd49d8c1c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 14 is out of bounds for axis 0 with size 14",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m test_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(all_indices, size \u001b[38;5;241m=\u001b[39m n_test, replace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m train_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msetdiff1d(all_indices, test_indices)\n\u001b[1;32m---> 15\u001b[0m X_train \u001b[38;5;241m=\u001b[39m X[train_indices]\n\u001b[0;32m     16\u001b[0m y_train \u001b[38;5;241m=\u001b[39m y[train_indices]\n\u001b[0;32m     17\u001b[0m X_test \u001b[38;5;241m=\u001b[39m X[test_indices]\n",
      "\u001b[1;31mIndexError\u001b[0m: index 14 is out of bounds for axis 0 with size 14"
     ]
    }
   ],
   "source": [
    "# Assume n, X, and y are already defined. For example:\n",
    "# n = X.shape[0]\n",
    "# (Make sure X is a NumPy array and y is a 1D array or similar.)\n",
    "k = 5 \n",
    "n_test = round(n * 1/k)\n",
    "n_train = n - n_test\n",
    "\n",
    "#Create an array of all indicie \n",
    "all_indices = np.arange(n)\n",
    "\n",
    "test_indices = np.random.choice(all_indices, size = n_test, replace = False)\n",
    "\n",
    "train_indices = np.setdiff1d(all_indices, test_indices)\n",
    "\n",
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "X_test = X[test_indices]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train length:\", len(y_train))\n",
    "print(\"y_test length:\", len(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e2cf23-efeb-430e-a04f-c361cde984f5",
   "metadata": {},
   "source": [
    "Fit an OLS model. Find the s_e in sample and out of sample. Which one is greater? Note: we are now using s_e and not RMSE since RMSE has the n-(p + 1) in the denominator not n-1 which attempts to de-bias the error estimate by inflating the estimate when overfitting in high p. Again, we're just using `sd(e)`, the sample standard deviation of the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80db879f-35ca-4023-9a30-7fa09ff1bcb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Fit the OLS model on the training set.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# (Assume X_train and y_train have already been defined,\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# and that X_train already includes a constant column if needed.)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39mOLS(y_train, X_train)\u001b[38;5;241m.\u001b[39mfit()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#in sample SE\u001b[39;00m\n\u001b[0;32m      7\u001b[0m in_sample_se \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(model\u001b[38;5;241m.\u001b[39mresid, ddof \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Fit the OLS model on the training set.\n",
    "# (Assume X_train and y_train have already been defined,\n",
    "# and that X_train already includes a constant column if needed.)\n",
    "model = sm.OLS(y_train, X_train).fit()\n",
    "\n",
    "#in sample SE\n",
    "in_sample_se = np.std(model.resid, ddof = 1)\n",
    "\n",
    "y_pred_test = model.predict(X_test)\n",
    "out_sample_resid = y_test - y_pred_test\n",
    "out_sample_se = np.std(out_sample_resid, ddof = 1)\n",
    "\n",
    "\n",
    "print(\"In-sample s_e (sample SD of residuals):\", in_sample_se)\n",
    "print(\"Out-of-sample s_e (sample SD of residuals):\", out_sample_se)\n",
    "\n",
    "# Typically, the out-of-sample s_e will be greater than the in-sample s_e."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f02f4a0-209b-466c-ab17-ec7474bcb2f4",
   "metadata": {},
   "source": [
    "Do these two exercises `Nsim = 1000` times and find the average difference between s_e and ooss_e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a50414fb-9dc6-4c73-a540-534699b1df68",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2145398403.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[20], line 9\u001b[1;36m\u001b[0m\n\u001b[1;33m    y_hat_test = model.predict(X_test)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Assume n, X, and y are already defined.\n",
    "# Here, n is the total number of observations.\n",
    "# X is the design matrix and y is the response vector.\n",
    "# For consistency with the R code (which uses '+0'), we will fit the model without a constant.\n",
    "# If X already has a constant column, you might need to remove it; here we assume X does NOT include one.\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_hat_test = model.predict(X_test)\n",
    "    oos_residuals = y_test - y_hat_test\n",
    "    \n",
    "    # Compute out-of-sample standard error (sample standard deviation of test residuals)\n",
    "    oosSSE_array[i] = np.std(oos_residuals, ddof=1)\n",
    "\n",
    "    # Compute in-sample standard error (sample standard deviation of residuals from the training fit)\n",
    "    s_e_array[i] = np.std(model.resid, ddof=1)\n",
    "\n",
    "\n",
    "# Compute the average difference between in-sample and out-of-sample s_e\n",
    "mean_diff = np.mean(s_e_array - oosSSE_array)\n",
    "print(\"Average difference (s_e - ooss_e):\", mean_diff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2aafa5f-d4c5-4fdb-a139-98e3a6154f86",
   "metadata": {},
   "source": [
    "We'll now add random junk to the data so that `p_plus_one = n_train` and create a new data matrix `X_with_junk.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b38a88-f31a-46e9-aedd-d2eec495f47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume X, n, n_train, and p_plus_one are already defined.\n",
    "# Create the junk matrix: with n rows and (n_train - p_plus_one) columns.\n",
    "p = X.shape[1]\n",
    "p_plus_one = p + 1\n",
    "num_junk = n_train - p_plus_one\n",
    "\n",
    "junk = np.random.randn(n, num_junk)\n",
    "\n",
    "# Concatenate X and the junk matrix horizontally\n",
    "X_with_junk = np.hstack((X, junk))\n",
    "\n",
    "# Print dimensions\n",
    "print(\"X shape:\", )\n",
    "print(\"X_with_junk shape:\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acafda18-b5bc-4911-aea4-90a2d1d27e3d",
   "metadata": {},
   "source": [
    "Repeat the exercise above measuring the average s_e and ooss_e but this time record these metrics by number of features used. That is, do it for the first column of `X_with_junk` (the intercept column), then do it for the first and second columns, then the first three columns, etc until you do it for all columns of `X_with_junk`. Save these in `s_e_by_p` and `ooss_e_by_p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef7cef0-29a4-48fe-81e8-7808dd9e011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5  # Test set is 1/5 of the data\n",
    "n_test = n/k\n",
    "n_train = n - n_test\n",
    "Nsim = 1000\n",
    "p_plus_one = X.shape[1] + 1\n",
    "num_junk = n_train - p_plus_one\n",
    "junk = np.random.randn(n, num_junk)\n",
    "X_with_junk = np.hstack((X, junk))\n",
    "\n",
    "num_features = X_with_junk.shape[1]\n",
    "\n",
    "\n",
    "\n",
    "# Loop over model sizes (using the first j columns of X_with_junk)\n",
    "for j in range(1, num_features + 1):\n",
    "    s_e_array = np.zeros(Nsim)\n",
    "    oosSSE_array = np.zeros(Nsim)\n",
    "    \n",
    "    for i in range(Nsim):\n",
    "        # Generate random test indices without replacement (0-indexed)\n",
    "        all_indices = np.arange(n)\n",
    "        test_indices = np.random.choice(all_indices, size=n_test, replace=False)\n",
    "        train_indices = np.setdiff1d(all_indices, test_indices)\n",
    "        \n",
    "        # Subset the data using the first j columns\n",
    "        X_train = X_with_junk[train_indices, :j]\n",
    "        X_test = X_with_junk[test_indices, :j]\n",
    "        y_train = y[train_indices]\n",
    "        y_test = y[test_indices]\n",
    "        \n",
    "        # Fit an OLS model with no intercept (like lm(... + 0) in R)\n",
    "        model = sm.OLS(y_train, X_train).fit()\n",
    "        \n",
    "        # Predict on the test set\n",
    "        y_hat_test = model.predict(X_test)\n",
    "        \n",
    "        # Compute the sample standard deviation (ddof=1) of residuals\n",
    "        oosSSE_array[i] = np.std(y_test - y_hat_test, ddof=1)\n",
    "        s_e_array[i] = np.std(model.resid, ddof=1)\n",
    "    \n",
    "    # Record the average errors for model size j\n",
    "     s_e_by_p[j-1] = np.mean(s_e_array)\n",
    "    ooss_e_by_p[j-1] = np.mean(oosSSE_array)\n",
    "\n",
    "print(\"Average out-of-sample s_e by number of features:\")\n",
    "print(ooss_e_by_p)\n",
    "print(\"Average in-sample s_e by number of features:\")\n",
    "print(s_e_by_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb20846e-3625-4081-9ea6-74e977e8af8e",
   "metadata": {},
   "source": [
    "You can graph them here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d689dae8-a476-46f0-9540-86cd78005c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from plotnine import ggplot, aes, geom_line, labs, theme_minimal\n",
    "p_values = np.arange(1, len(s_e_by_p) + 1)\n",
    "\n",
    "# Create data frames for the in-sample and out-of-sample standard errors.\n",
    "df_in = pd.DataFrame({\n",
    "    'p': p_values,\n",
    "    's_e': s_e_by_p,\n",
    "})\n",
    "df_out = pd.DataFrame({\n",
    "     'p': p_values,\n",
    "    's_e': ooss_e_by_p,\n",
    "    'series': 'Out-of-sample'\n",
    "})\n",
    "\n",
    "# Combine the two dataframes\n",
    "df = pd.concat( ignore_index=True)\n",
    "\n",
    "# Create the plot using plotnine\n",
    "plot = (ggplot(df, aes(x='p', y='s_e', color='series'))\n",
    "        + geom_line()\n",
    "        + labs(x='Number of features (p)',\n",
    "               y='s_e',\n",
    "               title='In-sample vs. Out-of-sample s_e by number of features')\n",
    "        + theme_minimal()\n",
    "       )\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503c5d83-d874-4717-8d94-89324d3537ba",
   "metadata": {},
   "source": [
    "Is this shape expected? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623ef063-4883-4258-997c-f017a5123742",
   "metadata": {},
   "source": [
    "Yes this shape is expected because as we add more features the in-sample error will decrease due to the the model fitting the additional features and data. However, the out of sample error will start to get worse due to the over-fitting that is occurring. This will lead to a worse modle that produces worse predictions for data that is out of sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8229ba1f-a813-467d-8a86-6ac3b9112cd3",
   "metadata": {},
   "source": [
    "Now repeat the exercise above except use 5-fold CV (K=5 cross validation) for each p. The code below will also plot the oos RMSE. This oos RMSE curve should be similar to the curve in the above problem, but now it will be more stable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13c8bc3-63a8-4dd7-90d9-5aa952b96aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume n, y, and X_with_junk are already defined.\n",
    "# Let p_max be the total number of features (columns) in X_with_junk.\n",
    "p_max = X_with_junk.shape[1]\n",
    "\n",
    "K = 5\n",
    "n_test = n/k\n",
    "\n",
    "# Create a folds array: assign each observation to one of K folds.\n",
    "# Replicate 1:K enough times, then shuffle.\n",
    "folds = np.tile()[:]\n",
    "np.random.shuffle()\n",
    "\n",
    "# Initialize a matrix to store the out-of-sample RMSE for each model size and fold.\n",
    "p_max = X_with_junk.shape[1]\n",
    "\n",
    "ooss_e_by_p_k = np.zeros((p_max, K))\n",
    "\n",
    "for j in range():\n",
    "    for k in range():\n",
    "        # Get test and train indices for fold k\n",
    "        test_indices = np.where(folds == k)[0]\n",
    "        train_indices = np.where(folds != k)[0]\n",
    "        \n",
    "        # Subset the data using the first j columns of X_with_junk\n",
    "        \n",
    "        X_train = X_with_junk[train_indices, :j]\n",
    "        y_train = y[train_indices]\n",
    "        X_test = X_with_junk[test_indices, :j]\n",
    "        y_test = y[test_indices]\n",
    "\n",
    "        \n",
    "        # Fit an OLS model with no intercept (mirroring R's '+ 0')\n",
    "        model = sm.OLS(y_train, X_train).fit()\n",
    "        y_hat = model.predict(X_test)\n",
    "\n",
    "        \n",
    "        # Compute RMSE for this fold and model size\n",
    "        ooss_e_by_p_k[] = np.sqrt(np.mean((y_test - y_hat)**2))\n",
    "\n",
    "# For each model size (row), compute the standard deviation of the RMSE over the K folds.\n",
    "s_e_by_p = np.std(ooss_e_by_p_k, axis=1)\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "df_plot = pd.DataFrame({\n",
    "    'p': np.arange(1, p_max + 1),\n",
    "    's_e': s_e_by_p\n",
    "})\n",
    "\n",
    "# Plot using plotnine\n",
    "plot = (ggplot(df_plot, aes(x='p', y='s_e'))\n",
    "        + geom_line()\n",
    "        + labs(x=\"Number of Features\", y=\"OOS RMSE (sd)\", title=\"5-fold CV OOS RMSE vs. Number of Features\")\n",
    "        + theme_minimal())\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee2321b-3537-4bb9-8b7e-f24fdf5c4419",
   "metadata": {},
   "source": [
    "Even though the concept of confidence intervals (CIs) will not be on the midterm, construct 95% CIs for each of the oosRMSE measurements by number of features, p. A CI is a real-number interval with a lower bound and upper bound. The formula for the CI is [s_e - 2 * s_s_e, s_e + 2 * s_s_e]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f25d85-53c0-4b1d-b4dc-6edb85664970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean out-of-sample RMSE for each model size (row-wise mean)\n",
    "mean_oos = np.mean(ooss_e_by_p_k)\n",
    "\n",
    "# Compute the sample standard deviation for each model size (across simulations)\n",
    "sd_oos = np.std(ooss_e_by_p_k)\n",
    "\n",
    "# Construct the 95% confidence intervals\n",
    "CI_lower = mean_oos - 2 * sd_oos\n",
    "CI_upper = mean_oos + 2 * sd_oos\n",
    "\n",
    "# Create a DataFrame with the number of features and the CI bounds.\n",
    "# Assuming n_train is the number of features (columns in X_with_junk)\n",
    "df_CI = pd.DataFrame({\n",
    "     'p': np.arange(1, X_with_junk.shape[1] + 1),\n",
    "    'mean_oos_RMSE': mean_oos,\n",
    "    'CI_lower': CI_lower,\n",
    "    'CI_upper': CI_upper\n",
    "})\n",
    "\n",
    "print(df_CI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
